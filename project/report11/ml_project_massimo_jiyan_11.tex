\documentclass[a4paper, 11pt]{article}
\usepackage{comment} % enables the use of multi-line comments (\ifx \fi) 
\usepackage{lipsum} %This package just generates Lorem Ipsum filler text. 
\usepackage{fullpage} % changes the margin
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}

\begin{document}
%Header-Make sure you update this information!!!!
\noindent
\large\textbf{Machine Learning Project report} \hfill \textbf{Jiyan PutYourLastNameHere} \\
\normalsize ML \hfill \textbf{Massimo Innocentini} \\
Dr. Norman Hendrich \\
TA: Philipp Ruppel \hfill Due Date: 30/06/2018

\section*{Introduction}
In the forest cover dataset the goal is to predict the type fo cover that a certain has based on the observation of the environment. The dataset provides multiple features like soil type, area elevation, distance from water as we described in the first report. We have 7 types of cover type and each row in the data frame is associated with only a single type, thus the problem is known as multi-class classification. The Kaggle competition website states that the model submitted is simply evaluated on the accuracy to classify the cover type on the test data, the value is returned as a percentage so 100\% is perfect accuracy and obviously 0\% the model is useless.

\section*{Data Preprocessing}


\section*{kNN}
minmaxscaler works way better than normalization
\section*{SVM}

\section*{Deep Neural Network}

\section*{Results}

\section*{Discussion \& Comparison}

\section*{Conclusion}

\begin{thebibliography}{9}

\end{thebibliography}

\end{document}